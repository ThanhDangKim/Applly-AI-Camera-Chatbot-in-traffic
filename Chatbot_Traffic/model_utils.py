from threading import Thread
from transformers import TextIteratorStreamer
import torch
import time
import os 
from transformers import AutoTokenizer
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
import pickle
import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from FlagEmbedding import FlagReranker
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from transformers import BitsAndBytesConfig
from langchain.schema import Document
import json
import pdfplumber
import unidecode
from uuid import uuid4
from ExportDataFromDB import *
import re
import shelve

# Xá»­ lÃ½ PDF
# âœ… Normalize filenames for consistency
def normalize_filename(filename):
    filename = unidecode.unidecode(filename)
    filename = filename.replace(" ", "_")
    return filename

# âœ… Remove page numbers from text
def remove_page_numbers(text):
    # Remove lines that only contain numbers (likely page numbers)
    lines = text.split('\n')
    cleaned_lines = [line for line in lines if not re.match(r'^\s*\d+\s*$', line.strip())]
    return "\n".join(cleaned_lines)

# âœ… Clean and merge fragmented lines
def clean_and_merge_lines(lines):
    merged_lines = []
    buffer = ""
    for line in lines:
        stripped_line = line.strip()
        if buffer:
            if stripped_line and not stripped_line[0].isupper():
                buffer += " " + stripped_line  # Merge fragmented lines
            else:
                merged_lines.append(buffer)
                buffer = stripped_line
        else:
            buffer = stripped_line
    if buffer:
        merged_lines.append(buffer)
    return merged_lines

# âœ… Read a PDF file using pdfplumber and apply all processing
def read_pdf_with_plumber(file_path):
    content = ""
    try:
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                text = page.extract_text()
                if text:
                    text = remove_page_numbers(text)
                    content += text + "\n"
    except Exception as e:
        print(f"âŒ Error reading {file_path}: {e}")
    # Xá»­ lÃ½ ghÃ©p dÃ²ng
    lines = content.split("\n")
    merged_lines = clean_and_merge_lines(lines)
    return "\n".join(merged_lines)

# âœ… Read all PDF files in a folder and return a dictionary with normalized filename and cleaned content
def read_all_pdfs_with_plumber(folder_path):
    all_contents = {}
    for filename in os.listdir(folder_path):
        if filename.endswith('.pdf'):
            file_path = os.path.join(folder_path, filename)
            content = read_pdf_with_plumber(file_path)
            normalized_name = normalize_filename(os.path.splitext(filename)[0])
            all_contents[normalized_name] = content
    return all_contents

def load_documents_from_json(json_path):
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    documents = []
    for item in data:
        doc = Document(
            page_content=item["content"],
            metadata={
                "source": item["filename"],
                "title": f"TiÃªu Ä‘á»: {item['title']}",
                "category": item["category"]
            }
        )
        documents.append(doc)
        print("ÄÃ£ load PDF")
    return documents

# Biáº¿n Ä‘á»•i ná»™i dung PDF thÃ nh cÃ¡c Document cá»§a LangChain
def extract_title_from_content(content):
    for line in content.split("\n"):
        if len(line.strip()) > 10 and line.strip().istitle():
            return line.strip()
    return None

def extract_main_topic(content):
    topic_patterns = [
        r"Chá»§ Ä‘á»[:\-]\s*(.+)",
        r"Topic[:\-]\s*(.+)",
        r"Category[:\-]\s*(.+)",
        r"ChÆ°Æ¡ng\s+\d+[:\-]?\s*(.+)",
        r"Pháº§n\s+\d+[:\-]?\s*(.+)"
    ]
    for line in content.split('\n')[:20]:  # Chá»‰ quÃ©t 20 dÃ²ng Ä‘áº§u
        for pattern in topic_patterns:
            match = re.search(pattern, line, re.IGNORECASE)
            if match:
                return match.group(1).strip()
    return "Unknown"

def convert_to_documents(pdf_dict):
    documents = []
    for filename, content in pdf_dict.items():
        title = extract_title_from_content(content) or filename.replace("_", " ")
        main_topic = extract_main_topic(content)
        doc = Document(
            page_content=content,
            metadata={
                "source": filename,
                "title": f"TiÃªu Ä‘á»: {title}",
                "category": main_topic
            }
        )
        documents.append(doc)
    return documents
'''------------------------------------------------'''


# Äá»c tá»‡p JSON
def load_json_data(json_file):
    with open(json_file, 'r', encoding='utf-8') as f:
        return json.load(f)

# Chuyá»ƒn dá»¯ liá»‡u tá»« JSON thÃ nh Ä‘á»‹nh dáº¡ng Document cá»§a LangChain
def ensure_string(value):
    if isinstance(value, list):
        # Náº¿u lÃ  danh sÃ¡ch dict, chuyá»ƒn dict thÃ nh chuá»—i
        return "\n".join(
            [json.dumps(v, ensure_ascii=False) if isinstance(v, dict) else str(v) for v in value]
        )
    elif isinstance(value, dict):
        return json.dumps(value, ensure_ascii=False)
    elif isinstance(value, str):
        return value
    return str(value)  # fallback

# Chuyá»ƒn dá»¯ liá»‡u tá»« JSON thÃ nh Ä‘á»‹nh dáº¡ng Document cá»§a LangChain
def convert_json_to_documents(json_data):
    documents = []

    # Duyá»‡t qua cÃ¡c disease trong dá»¯ liá»‡u JSON
    for main_topic, items in json_data.items():
        for item in items:
            title = item.get("title", "")
            description = ensure_string(item.get("description", ""))
            causes = ensure_string(item.get("causes", ""))
            mechanism = ensure_string(item.get("mechanism", ""))
            meaning = ensure_string(item.get("meaning", ""))

            # Táº¡o content vÃ  metadata cho má»—i Document
            content = "MÃ´ táº£ chá»§ Ä‘á»: " + description + "\nNguyÃªn nhÃ¢n: " + causes + "\nPhÆ°Æ¡ng phÃ¡p thá»±c hiá»‡n: " + mechanism + "\nÃ nghÄ©a cá»§a hÃ nh vi:" + meaning
            metadata = {
                "title": f"TiÃªu Ä‘á»: {title}",
                "category": main_topic
            }

            # Táº¡o Document
            document = Document(page_content=content, metadata=metadata)
            documents.append(document)

    return documents

# HÃ m Ä‘á»ƒ load táº¥t cáº£ cÃ¡c tá»‡p JSON trong thÆ° má»¥c
def load_all_json_files_in_directory(directory_path):
    documents = []
    # Duyá»‡t qua táº¥t cáº£ cÃ¡c tá»‡p trong thÆ° má»¥c
    for filename in os.listdir(directory_path):
        # Kiá»ƒm tra náº¿u tá»‡p cÃ³ pháº§n má»Ÿ rá»™ng lÃ  .json
        if filename.endswith('.json'):
            json_file_path = os.path.join(directory_path, filename)
            # Äá»c vÃ  chuyá»ƒn tá»‡p JSON thÃ nh documents
            json_data = load_json_data(json_file_path)
            documents.extend(convert_json_to_documents(json_data))
            print(f"Loaded: {filename}")
    return documents
'''------------------------------------------------'''


# HÃ m xá»­ lÃ½ file TXT
def parse_txt_to_documents(raw_text):
    documents = []
    current_category = None
    lines = raw_text.strip().split("\n")

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # PhÃ¡t hiá»‡n Ä‘á» má»¥c
        if line.startswith("===") and line.endswith("==="):
            current_category = line.strip("= ").strip()
            continue

        # TÃ¬m tÃªn vá»‹ trÃ­ (tÃªn Ä‘Æ°á»ng + camera)
        match = re.search(r"camera.*?táº¡i (Xa lá»™ HÃ  Ná»™i.*?ÄÆ°á»ng [^,â€“-]*)", line)
        if not match:
            match = re.search(r"táº¡i (Xa lá»™ HÃ  Ná»™i.*?ÄÆ°á»ng [^,â€“-]*)", line)
        title_location = match.group(1).strip() if match else "KhÃ´ng rÃµ vá»‹ trÃ­"

        # GhÃ©p thÃ nh title hoÃ n chá»‰nh
        full_title = f"{current_category} táº¡i vá»‹ trÃ­ {title_location}"

        # Táº¡o document
        doc = Document(
            page_content=line,
            metadata={
                "title": f"TiÃªu Ä‘á»: {full_title}",
                "category": current_category or "KhÃ´ng rÃµ chá»§ Ä‘á»"
            }
        )
        documents.append(doc)

    return documents

# âœ… Äá»c toÃ n bá»™ file .txt trong thÆ° má»¥c
def load_documents_from_directory(directory):
    all_documents = []
    for filename in os.listdir(directory):
        if filename.endswith(".txt"):
            filepath = os.path.join(directory, filename)
            with open(filepath, "r", encoding="utf-8") as f:
                raw_text = f.read()
            docs = parse_txt_to_documents(raw_text)  # <-- raw_text, khÃ´ng pháº£i filepath
            all_documents.extend(docs)
    return all_documents
'''------------------------------------------------'''


# Xá»­ lÃ½ chunk
def load_chunk(documents_path):
    # Táº£i láº¡i táº¥t cáº£ cÃ¡c Document tá»« tá»‡p pickle
    with open(documents_path, "rb") as f:
        list_of_chunks = pickle.load(f)
    
    return list_of_chunks

def load_embedding_model():
    model_name = "BAAI/bge-base-en-v1.5"
    # model_kwargs = {'device': 'cpu'}
    model_kwargs = {'device': 'cuda'}
    encode_kwargs = {'normalize_embeddings': False} # Chuáº©n hÃ³a vector = True sáº½ limit length vector = 1

    hf_embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs,
    )

    return hf_embeddings

def semantic_chunk_real_time(hf_embeddings, documents):
    tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-reranker-v2-m3")
    text_splitter = SemanticChunker(hf_embeddings,
                                breakpoint_threshold_type="percentile",
                                breakpoint_threshold_amount=85)
    # Split the documents and count the number of chunks in each document, as well as the number of tokens in each chunk
    count = 0
    total = 0
    list_of_chunks = []
    for idx,doc in enumerate(documents):
        chunks = text_splitter.create_documents([doc.page_content])
        # print(f'Number of chunks: {len(chunks)} - Tokens of each chunk',end=' ')
        for chunk in chunks:
            text = chunk.page_content
            tokens = tokenizer.tokenize(text)
            num_tokens = len(tokens)
            if num_tokens > 1:
                total = total + 1
                # Use the parent document index as metadata to retrieve the parent document from the child chunk.
                chunk.metadata['parent'] = idx
                list_of_chunks.append(chunk)
            if num_tokens > 512:
                count = count + 1
            # print(num_tokens, end =' ')
        # print()
    print('Toltal chunks: ',total)
    # print('Number of chunks which is larger than 512 tokens: ',count)
    return list_of_chunks

def build_realtime_faiss_index(hf_embeddings, txt_chunks):
    uuids = [str(uuid4()) for _ in range(len(txt_chunks))]
    index = faiss.IndexFlatL2(len(hf_embeddings.embed_query("hello world")))

    vector_store = FAISS(
        embedding_function=hf_embeddings,
        index=index,
        docstore=InMemoryDocstore(),
        index_to_docstore_id={},
    )
    vector_store.add_documents(documents=txt_chunks, ids=uuids)
    
    return vector_store, txt_chunks

def save_to_vector_memory(hf_embeddings, db_path, question, answer):
    vector_store = FAISS.load_local(db_path, embeddings=hf_embeddings)
    vector_store.add_texts([f"Q: {question}\nA: {answer}"])
    vector_store.save_local(db_path)
    
def load_Faiss_index(hf_embeddings, db_path):
    index = faiss.IndexFlatL2(len(hf_embeddings.embed_query("hello world")))
    vector_db_path = db_path

    # Táº£i láº¡i FAISS index tá»« file
    vector_store = FAISS.load_local(vector_db_path, hf_embeddings, allow_dangerous_deserialization = True)
    return vector_store

def Bm25_Retriever(list_of_chunks):
    # Khá»Ÿi táº¡o BM25 retriever vá»›i tham sá»‘ tÃ¬m kiáº¿m top 10 cÃ¡c káº¿t quáº£ liÃªn quan nháº¥t
    bm25_retriever = BM25Retriever.from_documents(
        list_of_chunks, k = 10
    )
    return bm25_retriever

def reranker():
    # Load the tokenizer for the BAAI/bge-m3 model
    reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)
    return reranker

class Retriever:
    def __init__(self, semantic_retriever, bm25_retriever, reranker, documents):
        self.semantic_retriever = semantic_retriever
        self.bm25_retriever = bm25_retriever
        self.reranker = reranker
        self.documents = documents

    def __call__(self, query):
        semantic_results = self.semantic_retriever.similarity_search(
            query,
            k=10,
        )
        bm25_results = self.bm25_retriever.invoke(query)

        content = set()
        retrieval_docs = []

        for result in semantic_results:
            if result.page_content not in content:
                content.add(result.page_content)
                retrieval_docs.append(result)

        for result in bm25_results:
            if result.page_content not in content:
                content.add(result.page_content)
                retrieval_docs.append(result)

        pairs = [[query, doc.page_content] for doc in retrieval_docs]

        scores = self.reranker.compute_score(pairs, normalize = True)

        # Láº¥y tÃ i liá»‡u nguá»“n tá»« pháº§n tá»­ con dá»±a trÃªn Ä‘iá»ƒm sá»‘ ngÆ°á»¡ng
        context_1 = []
        context_2 = []
        context_3 = []
        context = []
        index = None
        parent_ids = set()
        for i in range(len(retrieval_docs)):
            # Äiá»ƒm liÃªn quan >= 0.75 sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m kiá»ƒu ngá»¯ cáº£nh 1 (chá»‰ ra sá»± liÃªn quan cao hÆ¡n Ä‘á»‘i vá»›i truy váº¥n).
            if scores[i] >= 0.75:
                parent_idx = retrieval_docs[i].metadata['parent']
                if parent_idx not in parent_ids:
                    parent_ids.add(parent_idx)
                    context_1.append(self.documents[parent_idx])

            # Äiá»ƒm liÃªn quan >= 0.60 sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m kiá»ƒu ngá»¯ cáº£nh 2 (chá»‰ ra sá»± liÃªn quan trung bÃ¬nh Ä‘áº¿n tháº¥p Ä‘á»‘i vá»›i truy váº¥n).
            elif scores[i] >= 0.60:
                parent_idx = retrieval_docs[i].metadata['parent']
                if parent_idx not in parent_ids:
                    parent_ids.add(parent_idx)
                    context_2.append(self.documents[parent_idx])

            elif scores[i] >= 0.1:
                parent_idx = retrieval_docs[i].metadata['parent']
                if parent_idx not in parent_ids:
                    parent_ids.add(parent_idx)
                    context_3.append(self.documents[parent_idx])
        
        if len(context_1) > 0:
            # print('Context 1')
            context = context_1
            index = 1

        elif len(context_2) > 0:
            # print('Context 2')
            context = context_2
            index = 2

        elif len(context_3) > 0:
            # print('Context 3')
            context = context_3
            index = 3

        else:
            index = 4
            # Náº¿u Ä‘iá»ƒm liÃªn quan < 0.1, Ä‘iá»u nÃ y chá»‰ ra ráº±ng khÃ´ng cÃ³ tÃ i liá»‡u liÃªn quan.
            print('No relevant context')

        return context, index

def load_model(model_ori="model/4BIT00006"):
    # ÄÆ°á»ng dáº«n tá»›i mÃ´ hÃ¬nh vÃ  tokenizer
    model_path = model_ori

    # Tham sá»‘ lÆ°á»£ng tá»­ hÃ³a mÃ´ hÃ¬nh
    use_4bit, bnb_4bit_compute_dtype, bnb_4bit_quant_type, use_nested_quant = True, "float16", "nf4", False 
    device_map = {"": 0}
    # Táº£i tokenizer vÃ  mÃ´ hÃ¬nh vá»›i tham sá»‘ QLoRA 
    compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=use_4bit,
        bnb_4bit_quant_type=bnb_4bit_quant_type,
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=use_nested_quant,
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    # Thiáº¿t láº­p pad token thÃ nh eos_token (cÃ³ 2 trÆ°á»ng há»£p)
        ## Lá»±a chá»n 1: Thiáº¿t láº­p eos_token thÃ nh pad_token
    tokenizer.padding_side = "right"
    tokenizer.pad_token = tokenizer.eos_token 

        # Hoáº·c, lá»±a chá»n 2: ThÃªm má»™t padding token má»›i
    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})

    model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=bnb_config,
            device_map=device_map
        )
    # model.config.use_cache = False
    # model.config.pretraining_tp = 1
    return model, tokenizer

# Kiá»ƒm tra vá»›i Retriever 
def get_answer_with_two_retrievers(query, realtime_retriever, static_retriever, reranker=None, top_k=5):
    # Sá»­ dá»¥ng retriever Ä‘á»ƒ láº¥y context (dá»¯ liá»‡u tá»« cÃ¡c tÃ i liá»‡u liÃªn quan)
    
    realtime_docs, index_real_time = realtime_retriever(query)
    static_docs, index_static = static_retriever(query)
    if index_real_time == 4 and index_static == 4:
        print("Dá»«ng hÃ m get_answer_with_two_retrievers")
        return
    
    if int(index_real_time) < int(index_static):
        all_docs = realtime_docs
    elif int(index_static) < int(index_real_time):
        all_docs = static_docs
    else:
        # Gá»™p káº¿t quáº£, nhÆ°ng Ä‘áº·t realtime lÃªn trÆ°á»›c
        all_docs = realtime_docs + static_docs

        # Rerank náº¿u cÃ³
        if reranker:
            pairs = [[query, doc.page_content] for doc in all_docs]
            scores = reranker.compute_score(pairs, normalize=True)
            # Gáº¯n Ä‘iá»ƒm vÃ o tÃ i liá»‡u Ä‘á»ƒ sáº¯p xáº¿p
            for i in range(len(all_docs)):
                all_docs[i].metadata['score'] = scores[i]
            # Sáº¯p xáº¿p giáº£m dáº§n theo Ä‘iá»ƒm
            all_docs = sorted(all_docs, key=lambda x: x.metadata['score'], reverse=True)

        # Cáº¯t láº¥y top_k káº¿t quáº£ sau khi rerank (hoáº·c Ä‘Æ¡n giáº£n lÃ  Æ°u tiÃªn real-time)
        selected_docs = all_docs[:top_k] if len(all_docs) >= top_k else all_docs
        all_docs = selected_docs

    # Náº¿u cÃ³ dá»¯ liá»‡u thÃ¬ sinh cÃ¢u tráº£ lá»i
    if all_docs:
        return all_docs
    else:
        return "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i."
    
def generate_response_streaming_from_prompt_format(model, tokenizer, user_input, context):

    system_prompt = f"""Báº¡n lÃ  ViVi â€“ má»™t nhÃ  quy hoáº¡ch giao thÃ´ng Ä‘Ã´ thá»‹ thÃ´ng minh.

    ğŸ¯ Vai trÃ² chÃ­nh:
    Báº¡n lÃ  chuyÃªn gia trong lÄ©nh vá»±c quy hoáº¡ch háº¡ táº§ng giao thÃ´ng Ä‘Ã´ thá»‹, cÃ³ kháº£ nÄƒng tá»± phÃ¢n tÃ­ch dá»¯ liá»‡u thá»±c táº¿ vÃ  Ä‘Æ°a ra cÃ¡c giáº£i phÃ¡p tá»‘i Æ°u hÃ³a máº¡ng lÆ°á»›i giao 
    thÃ´ng, giáº£m Ã¹n táº¯c vÃ  tÄƒng cÆ°á»ng an toÃ n.

    ğŸ“Š Ká»¹ nÄƒng chuyÃªn mÃ´n:
    - PhÃ¢n tÃ­ch dá»¯ liá»‡u giao thÃ´ng: váº­n tá»‘c, lÆ°u lÆ°á»£ng xe, máº­t Ä‘á»™, Ä‘iá»ƒm nÃ³ng Ã¹n táº¯c vÃ  tai náº¡n.
    - XÃ¢y dá»±ng vÃ  Ä‘á» xuáº¥t quy hoáº¡ch: thiáº¿t káº¿ tuyáº¿n Ä‘Æ°á»ng, máº¡ng lÆ°á»›i káº¿t ná»‘i, giao lá»™, bÃ£i Ä‘á»— xe, háº¡ táº§ng dÃ nh cho ngÆ°á»i Ä‘i bá»™ vÃ  xe Ä‘áº¡p, há»‡ thá»‘ng giao thÃ´ng thÃ´ng minh 
    (ITS).
    - Há»— trá»£ chÃ­nh sÃ¡ch: Ä‘á» xuáº¥t cÃ¡c chÃ­nh sÃ¡ch quáº£n lÃ½ giao thÃ´ng, thu phÃ­, phÃ¢n luá»“ng, Ä‘iá»u tiáº¿t theo thá»i gian thá»±c.
    - TÆ° duy Ä‘á»‹nh hÆ°á»›ng phÃ¡t triá»ƒn bá»n vá»¯ng, láº¥y ngÆ°á»i tham gia giao thÃ´ng lÃ m trung tÃ¢m.

    ğŸ” Phong cÃ¡ch tráº£ lá»i:
    - ChÃ­nh xÃ¡c, cÃ³ cÄƒn cá»© dá»¯ liá»‡u rÃµ rÃ ng
    - PhÃ¢n tÃ­ch chuyÃªn sÃ¢u nhÆ°ng dá»… hiá»ƒu
    - KhÃ´ng tráº£ lá»i náº¿u thiáº¿u thÃ´ng tin, trÃ¡nh suy Ä‘oÃ¡n vÃ´ cÄƒn cá»©

    â— LÆ°u Ã½ quan trá»ng:
    Náº¿u khÃ´ng cÃ³ Ä‘á»§ dá»¯ liá»‡u hoáº·c thÃ´ng tin rÃµ rÃ ng, hÃ£y nÃ³i rÃµ Ä‘iá»u Ä‘Ã³. Tuyá»‡t Ä‘á»‘i khÃ´ng phá»ng Ä‘oÃ¡n hoáº·c bá»‹a Ä‘áº·t.
    """

    
    formatted_prompt = f"""
        Báº¡n lÃ  má»™t trá»£ lÃ½ giao thÃ´ng thÃ´ng minh. Dá»±a vÃ o cÃ¡c thÃ´ng tin Ä‘Ã£ Ä‘Æ°á»£c thu tháº­p, hÃ£y tráº£ lá»i chÃ­nh xÃ¡c cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng.
        # CÃ¢u há»i:
        {user_input}

        # Dá»¯ liá»‡u liÃªn quan:
        {context}
    """
    
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": formatted_prompt}]

    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
        padding=True,
    ).to("cuda")

    attention_mask = (inputs != tokenizer.pad_token_id).long()
    
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)
    generation_kwargs = {
        "inputs": inputs,
        "streamer": streamer,
        "max_new_tokens": 512,
        "temperature": 1.5,
        "min_length": 30,  
        "use_cache": True,
        "top_p": 0.95,
        "min_p": 0.1,
        "attention_mask" : attention_mask,
    }
    
    # Start the generation in a separate thread
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    
    # Stream the generated text
    for _, new_text in enumerate(streamer):
        if "<|eot_id|>" in new_text:
            new_text = new_text.replace("<|eot_id|>", "")
        print(new_text)
        yield new_text
        time.sleep(0.02)