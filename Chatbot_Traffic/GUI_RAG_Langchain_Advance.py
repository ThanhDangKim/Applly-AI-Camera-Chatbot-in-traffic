import streamlit as st
from threading import Thread
from transformers import TextIteratorStreamer
import torch
import time
import os 
from transformers import AutoTokenizer
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
import pickle
import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from FlagEmbedding import FlagReranker
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from transformers import BitsAndBytesConfig
from langchain.schema import Document
import json
import pdfplumber
import unidecode
from uuid import uuid4
from ExportDataFromDB import *
import re

st.set_page_config(
    page_title="TrafficChat ViVi",
    page_icon="üö¶",
    # layout="span",
    initial_sidebar_state="collapsed"
)

# X·ª≠ l√Ω PDF
# ‚úÖ Normalize filenames for consistency
def normalize_filename(filename):
    filename = unidecode.unidecode(filename)
    filename = filename.replace(" ", "_")
    return filename

# ‚úÖ Remove page numbers from text
def remove_page_numbers(text):
    # Remove lines that only contain numbers (likely page numbers)
    lines = text.split('\n')
    cleaned_lines = [line for line in lines if not re.match(r'^\s*\d+\s*$', line.strip())]
    return "\n".join(cleaned_lines)

# ‚úÖ Clean and merge fragmented lines
def clean_and_merge_lines(lines):
    merged_lines = []
    buffer = ""
    for line in lines:
        stripped_line = line.strip()
        if buffer:
            if stripped_line and not stripped_line[0].isupper():
                buffer += " " + stripped_line  # Merge fragmented lines
            else:
                merged_lines.append(buffer)
                buffer = stripped_line
        else:
            buffer = stripped_line
    if buffer:
        merged_lines.append(buffer)
    return merged_lines

# ‚úÖ Read a PDF file using pdfplumber and apply all processing
def read_pdf_with_plumber(file_path):
    content = ""
    try:
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                text = page.extract_text()
                if text:
                    text = remove_page_numbers(text)
                    content += text + "\n"
    except Exception as e:
        print(f"‚ùå Error reading {file_path}: {e}")
    # X·ª≠ l√Ω gh√©p d√≤ng
    lines = content.split("\n")
    merged_lines = clean_and_merge_lines(lines)
    return "\n".join(merged_lines)

# ‚úÖ Read all PDF files in a folder and return a dictionary with normalized filename and cleaned content
def read_all_pdfs_with_plumber(folder_path):
    all_contents = {}
    for filename in os.listdir(folder_path):
        if filename.endswith('.pdf'):
            file_path = os.path.join(folder_path, filename)
            content = read_pdf_with_plumber(file_path)
            normalized_name = normalize_filename(os.path.splitext(filename)[0])
            all_contents[normalized_name] = content
    return all_contents

def load_documents_from_json(json_path):
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    documents = []
    for item in data:
        doc = Document(
            page_content=item["content"],
            metadata={
                "source": item["filename"],
                "title": f"Ti√™u ƒë·ªÅ: {item['title']}",
                "category": item["category"]
            }
        )
        documents.append(doc)
    return documents

# Bi·∫øn ƒë·ªïi n·ªôi dung PDF th√†nh c√°c Document c·ªßa LangChain
def extract_title_from_content(content):
    for line in content.split("\n"):
        if len(line.strip()) > 10 and line.strip().istitle():
            return line.strip()
    return None

def extract_main_topic(content):
    topic_patterns = [
        r"Ch·ªß ƒë·ªÅ[:\-]\s*(.+)",
        r"Topic[:\-]\s*(.+)",
        r"Category[:\-]\s*(.+)",
        r"Ch∆∞∆°ng\s+\d+[:\-]?\s*(.+)",
        r"Ph·∫ßn\s+\d+[:\-]?\s*(.+)"
    ]
    for line in content.split('\n')[:20]:  # Ch·ªâ qu√©t 20 d√≤ng ƒë·∫ßu
        for pattern in topic_patterns:
            match = re.search(pattern, line, re.IGNORECASE)
            if match:
                return match.group(1).strip()
    return "Unknown"

def convert_to_documents(pdf_dict):
    documents = []
    for filename, content in pdf_dict.items():
        title = extract_title_from_content(content) or filename.replace("_", " ")
        main_topic = extract_main_topic(content)
        doc = Document(
            page_content=content,
            metadata={
                "source": filename,
                "title": f"Ti√™u ƒë·ªÅ: {title}",
                "category": main_topic
            }
        )
        documents.append(doc)
    return documents
'''------------------------------------------------'''


# ƒê·ªçc t·ªáp JSON
def load_json_data(json_file):
    with open(json_file, 'r', encoding='utf-8') as f:
        return json.load(f)

# Chuy·ªÉn d·ªØ li·ªáu t·ª´ JSON th√†nh ƒë·ªãnh d·∫°ng Document c·ªßa LangChain
def ensure_string(value):
    if isinstance(value, list):
        # N·∫øu l√† danh s√°ch dict, chuy·ªÉn dict th√†nh chu·ªói
        return "\n".join(
            [json.dumps(v, ensure_ascii=False) if isinstance(v, dict) else str(v) for v in value]
        )
    elif isinstance(value, dict):
        return json.dumps(value, ensure_ascii=False)
    elif isinstance(value, str):
        return value
    return str(value)  # fallback

# Chuy·ªÉn d·ªØ li·ªáu t·ª´ JSON th√†nh ƒë·ªãnh d·∫°ng Document c·ªßa LangChain
def convert_json_to_documents(json_data):
    documents = []

    # Duy·ªát qua c√°c disease trong d·ªØ li·ªáu JSON
    for main_topic, items in json_data.items():
        for item in items:
            title = item.get("title", "")
            description = ensure_string(item.get("description", ""))
            causes = ensure_string(item.get("causes", ""))
            mechanism = ensure_string(item.get("mechanism", ""))
            meaning = ensure_string(item.get("meaning", ""))

            # T·∫°o content v√† metadata cho m·ªói Document
            content = "M√¥ t·∫£ ch·ªß ƒë·ªÅ: " + description + "\nNguy√™n nh√¢n: " + causes + "\nPh∆∞∆°ng ph√°p th·ª±c hi·ªán: " + mechanism + "\n√ù nghƒ©a c·ªßa h√†nh vi:" + meaning
            metadata = {
                "title": f"Ti√™u ƒë·ªÅ: {title}",
                "category": main_topic
            }

            # T·∫°o Document
            document = Document(page_content=content, metadata=metadata)
            documents.append(document)

    return documents

# H√†m ƒë·ªÉ load t·∫•t c·∫£ c√°c t·ªáp JSON trong th∆∞ m·ª•c
def load_all_json_files_in_directory(directory_path):
    documents = []
    # Duy·ªát qua t·∫•t c·∫£ c√°c t·ªáp trong th∆∞ m·ª•c
    for filename in os.listdir(directory_path):
        # Ki·ªÉm tra n·∫øu t·ªáp c√≥ ph·∫ßn m·ªü r·ªông l√† .json
        if filename.endswith('.json'):
            json_file_path = os.path.join(directory_path, filename)
            # ƒê·ªçc v√† chuy·ªÉn t·ªáp JSON th√†nh documents
            json_data = load_json_data(json_file_path)
            documents.extend(convert_json_to_documents(json_data))
            print(f"Loaded: {filename}")
    return documents
'''------------------------------------------------'''


# H√†m x·ª≠ l√Ω file TXT
def parse_txt_to_documents(raw_text):
    documents = []
    current_category = None
    lines = raw_text.strip().split("\n")

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # Ph√°t hi·ªán ƒë·ªÅ m·ª•c
        if line.startswith("===") and line.endswith("==="):
            current_category = line.strip("= ").strip()
            continue

        # T√¨m t√™n v·ªã tr√≠ (t√™n ƒë∆∞·ªùng + camera)
        match = re.search(r"camera.*?t·∫°i (Xa l·ªô H√† N·ªôi.*?ƒê∆∞·ªùng [^,‚Äì-]*)", line)
        if not match:
            match = re.search(r"t·∫°i (Xa l·ªô H√† N·ªôi.*?ƒê∆∞·ªùng [^,‚Äì-]*)", line)
        title_location = match.group(1).strip() if match else "Kh√¥ng r√µ v·ªã tr√≠"

        # Gh√©p th√†nh title ho√†n ch·ªânh
        full_title = f"{current_category} t·∫°i v·ªã tr√≠ {title_location}"

        # T·∫°o document
        doc = Document(
            page_content=line,
            metadata={
                "title": f"Ti√™u ƒë·ªÅ: {full_title}",
                "category": current_category or "Kh√¥ng r√µ ch·ªß ƒë·ªÅ"
            }
        )
        documents.append(doc)

    return documents

# ‚úÖ ƒê·ªçc to√†n b·ªô file .txt trong th∆∞ m·ª•c
def load_documents_from_directory(directory):
    all_documents = []
    for filename in os.listdir(directory):
        if filename.endswith(".txt"):
            filepath = os.path.join(directory, filename)
            with open(filepath, "r", encoding="utf-8") as f:
                raw_text = f.read()
            docs = parse_txt_to_documents(raw_text)  # <-- raw_text, kh√¥ng ph·∫£i filepath
            all_documents.extend(docs)
    return all_documents
'''------------------------------------------------'''


# X·ª≠ l√Ω chunk
def load_chunk(documents_path):
    # T·∫£i l·∫°i t·∫•t c·∫£ c√°c Document t·ª´ t·ªáp pickle
    with open(documents_path, "rb") as f:
        list_of_chunks = pickle.load(f)
    
    return list_of_chunks

def load_embedding_model():
    model_name = "BAAI/bge-base-en-v1.5"
    # model_kwargs = {'device': 'cpu'}
    model_kwargs = {'device': 'cuda'}
    encode_kwargs = {'normalize_embeddings': False} # Chu·∫©n h√≥a vector = True s·∫Ω limit length vector = 1

    hf_embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs,
    )

    return hf_embeddings

def semantic_chunk_real_time(hf_embeddings, documents):
    tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-reranker-v2-m3")
    text_splitter = SemanticChunker(hf_embeddings,
                                breakpoint_threshold_type="percentile",
                                breakpoint_threshold_amount=85)
    # Split the documents and count the number of chunks in each document, as well as the number of tokens in each chunk
    count = 0
    total = 0
    list_of_chunks = []
    for idx,doc in enumerate(documents):
        chunks = text_splitter.create_documents([doc.page_content])
        print(f'Number of chunks: {len(chunks)} - Tokens of each chunk',end=' ')
        for chunk in chunks:
            text = chunk.page_content
            tokens = tokenizer.tokenize(text)
            num_tokens = len(tokens)
            if num_tokens > 1:
                total = total + 1
                # Use the parent document index as metadata to retrieve the parent document from the child chunk.
                chunk.metadata['parent'] = idx
                list_of_chunks.append(chunk)
            if num_tokens > 512:
                count = count + 1
            print(num_tokens, end =' ')
        print()
    print('Toltal chunks: ',total)
    print('Number of chunks which is larger than 512 tokens: ',count)
    return list_of_chunks

def build_realtime_faiss_index(hf_embeddings, txt_chunks):
    uuids = [str(uuid4()) for _ in range(len(txt_chunks))]
    index = faiss.IndexFlatL2(len(hf_embeddings.embed_query("hello world")))

    vector_store = FAISS(
        embedding_function=hf_embeddings,
        index=index,
        docstore=InMemoryDocstore(),
        index_to_docstore_id={},
    )
    vector_store.add_documents(documents=txt_chunks, ids=uuids)
    
    return vector_store, txt_chunks
    
def load_Faiss_index(hf_embeddings, db_path):
    index = faiss.IndexFlatL2(len(hf_embeddings.embed_query("hello world")))
    vector_db_path = db_path

    # T·∫£i l·∫°i FAISS index t·ª´ file
    vector_store = FAISS.load_local(vector_db_path, hf_embeddings, allow_dangerous_deserialization = True)
    return vector_store

def Bm25_Retriever(list_of_chunks):
    # Kh·ªüi t·∫°o BM25 retriever v·ªõi tham s·ªë t√¨m ki·∫øm top 10 c√°c k·∫øt qu·∫£ li√™n quan nh·∫•t
    bm25_retriever = BM25Retriever.from_documents(
        list_of_chunks, k = 10
    )
    return bm25_retriever

def reranker():
    # Load the tokenizer for the BAAI/bge-m3 model
    reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)
    return reranker

class Retriever:
    def __init__(self, semantic_retriever, bm25_retriever, reranker, documents):
        self.semantic_retriever = semantic_retriever
        self.bm25_retriever = bm25_retriever
        self.reranker = reranker
        self.documents = documents

    def __call__(self, query):
        semantic_results = self.semantic_retriever.similarity_search(
            query,
            k=10,
        )
        bm25_results = self.bm25_retriever.invoke(query)

        content = set()
        retrieval_docs = []

        for result in semantic_results:
            if result.page_content not in content:
                content.add(result.page_content)
                retrieval_docs.append(result)

        for result in bm25_results:
            if result.page_content not in content:
                content.add(result.page_content)
                retrieval_docs.append(result)

        pairs = [[query, doc.page_content] for doc in retrieval_docs]

        scores = self.reranker.compute_score(pairs, normalize = True)

        # L·∫•y t√†i li·ªáu ngu·ªìn t·ª´ ph·∫ßn t·ª≠ con d·ª±a tr√™n ƒëi·ªÉm s·ªë ng∆∞·ª°ng
        context_1 = []
        context_2 = []
        context_3 = []
        context = []
        index = None
        parent_ids = set()
        for i in range(len(retrieval_docs)):
            # ƒêi·ªÉm li√™n quan >= 0.75 s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng l√†m ki·ªÉu ng·ªØ c·∫£nh 1 (ch·ªâ ra s·ª± li√™n quan cao h∆°n ƒë·ªëi v·ªõi truy v·∫•n).
            if scores[i] >= 0.75:
                parent_idx = retrieval_docs[i].metadata['parent']
                if parent_idx not in parent_ids:
                    parent_ids.add(parent_idx)
                    context_1.append(self.documents[parent_idx])

            # ƒêi·ªÉm li√™n quan >= 0.60 s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng l√†m ki·ªÉu ng·ªØ c·∫£nh 2 (ch·ªâ ra s·ª± li√™n quan trung b√¨nh ƒë·∫øn th·∫•p ƒë·ªëi v·ªõi truy v·∫•n).
            elif scores[i] >= 0.60:
                parent_idx = retrieval_docs[i].metadata['parent']
                if parent_idx not in parent_ids:
                    parent_ids.add(parent_idx)
                    context_2.append(self.documents[parent_idx])

            elif scores[i] >= 0.1:
                parent_idx = retrieval_docs[i].metadata['parent']
                if parent_idx not in parent_ids:
                    parent_ids.add(parent_idx)
                    context_3.append(self.documents[parent_idx])
        
        if len(context_1) > 0:
            print('Context 1')
            context = context_1
            index = 1

        elif len(context_2) > 0:
            print('Context 2')
            context = context_2
            index = 2

        elif len(context_3) > 0:
            print('Context 3')
            context = context_3
            index = 3

        else:
            index = 4
            # N·∫øu ƒëi·ªÉm li√™n quan < 0.1, ƒëi·ªÅu n√†y ch·ªâ ra r·∫±ng kh√¥ng c√≥ t√†i li·ªáu li√™n quan.
            print('No relevant context')

        return context, index

def load_model():
    # ƒê∆∞·ªùng d·∫´n t·ªõi m√¥ h√¨nh v√† tokenizer
    model_path = r"model/4BIT00006"

    # Tham s·ªë l∆∞·ª£ng t·ª≠ h√≥a m√¥ h√¨nh
    use_4bit, bnb_4bit_compute_dtype, bnb_4bit_quant_type, use_nested_quant = True, "float16", "nf4", False 
    device_map = {"": 0}
    # T·∫£i tokenizer v√† m√¥ h√¨nh v·ªõi tham s·ªë QLoRA 
    compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=use_4bit,
        bnb_4bit_quant_type=bnb_4bit_quant_type,
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=use_nested_quant,
    )

    tokenizer = AutoTokenizer.from_pretrained(model_path)
    # Thi·∫øt l·∫≠p pad token th√†nh eos_token (c√≥ 2 tr∆∞·ªùng h·ª£p)
        ## L·ª±a ch·ªçn 1: Thi·∫øt l·∫≠p eos_token th√†nh pad_token
    tokenizer.padding_side = "right"
    tokenizer.pad_token = tokenizer.eos_token 

        # Ho·∫∑c, l·ª±a ch·ªçn 2: Th√™m m·ªôt padding token m·ªõi
    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})

    model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=bnb_config,
            device_map=device_map
        )

    # model.config.use_cache = False
    # model.config.pretraining_tp = 1
    return model, tokenizer

# Ki·ªÉm tra v·ªõi Retriever 
def get_answer_with_two_retrievers(query, realtime_retriever, static_retriever, reranker=None, top_k=5):
    # S·ª≠ d·ª•ng retriever ƒë·ªÉ l·∫•y context (d·ªØ li·ªáu t·ª´ c√°c t√†i li·ªáu li√™n quan)
    
    realtime_docs, index_real_time = realtime_retriever(query)
    static_docs, index_static = static_retriever(query)
    if index_real_time == 4 and  index_static == 4:
        print("D·ª´ng h√†m get_answer_with_two_retrievers")
        return
    
    if int(index_real_time) < int(index_static):
        all_docs = realtime_docs
    elif int(index_static) < int(index_real_time):
        all_docs = static_docs
    else:
        # G·ªôp k·∫øt qu·∫£, nh∆∞ng ƒë·∫∑t realtime l√™n tr∆∞·ªõc
        all_docs = realtime_docs + static_docs

        # Rerank n·∫øu c√≥
        if reranker:
            pairs = [[query, doc.page_content] for doc in all_docs]
            scores = reranker.compute_score(pairs, normalize=True)
            # G·∫Øn ƒëi·ªÉm v√†o t√†i li·ªáu ƒë·ªÉ s·∫Øp x·∫øp
            for i in range(len(all_docs)):
                all_docs[i].metadata['score'] = scores[i]
            # S·∫Øp x·∫øp gi·∫£m d·∫ßn theo ƒëi·ªÉm
            all_docs = sorted(all_docs, key=lambda x: x.metadata['score'], reverse=True)

        # C·∫Øt l·∫•y top_k k·∫øt qu·∫£ sau khi rerank (ho·∫∑c ƒë∆°n gi·∫£n l√† ∆∞u ti√™n real-time)
        selected_docs = all_docs[:top_k] if len(all_docs) >= top_k else all_docs
        all_docs = selected_docs

    # N·∫øu c√≥ d·ªØ li·ªáu th√¨ sinh c√¢u tr·∫£ l·ªùi
    if all_docs:
        return all_docs
    else:
        return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi."
    
def generate_response_streaming_from_prompt_format(model, tokenizer, user_input, context):

    system_prompt = f"""B·∫°n l√† ViVi ‚Äì m·ªôt nh√† quy ho·∫°ch giao th√¥ng ƒë√¥ th·ªã th√¥ng minh.

    üéØ Vai tr√≤ ch√≠nh:
    B·∫°n l√† chuy√™n gia trong lƒ©nh v·ª±c quy ho·∫°ch h·∫° t·∫ßng giao th√¥ng ƒë√¥ th·ªã, c√≥ kh·∫£ nƒÉng t·ª± ph√¢n t√≠ch d·ªØ li·ªáu th·ª±c t·∫ø v√† ƒë∆∞a ra c√°c gi·∫£i ph√°p t·ªëi ∆∞u h√≥a m·∫°ng l∆∞·ªõi giao th√¥ng, gi·∫£m √πn t·∫Øc v√† tƒÉng c∆∞·ªùng an to√†n.

    üìä K·ªπ nƒÉng chuy√™n m√¥n:
    - Ph√¢n t√≠ch d·ªØ li·ªáu giao th√¥ng: v·∫≠n t·ªëc, l∆∞u l∆∞·ª£ng xe, m·∫≠t ƒë·ªô, ƒëi·ªÉm n√≥ng √πn t·∫Øc v√† tai n·∫°n.
    - X√¢y d·ª±ng v√† ƒë·ªÅ xu·∫•t quy ho·∫°ch: thi·∫øt k·∫ø tuy·∫øn ƒë∆∞·ªùng, m·∫°ng l∆∞·ªõi k·∫øt n·ªëi, giao l·ªô, b√£i ƒë·ªó xe, h·∫° t·∫ßng d√†nh cho ng∆∞·ªùi ƒëi b·ªô v√† xe ƒë·∫°p, h·ªá th·ªëng giao th√¥ng th√¥ng minh (ITS).
    - H·ªó tr·ª£ ch√≠nh s√°ch: ƒë·ªÅ xu·∫•t c√°c ch√≠nh s√°ch qu·∫£n l√Ω giao th√¥ng, thu ph√≠, ph√¢n lu·ªìng, ƒëi·ªÅu ti·∫øt theo th·ªùi gian th·ª±c.
    - T∆∞ duy ƒë·ªãnh h∆∞·ªõng ph√°t tri·ªÉn b·ªÅn v·ªØng, l·∫•y ng∆∞·ªùi tham gia giao th√¥ng l√†m trung t√¢m.

    üîç Phong c√°ch tr·∫£ l·ªùi:
    - Ch√≠nh x√°c, c√≥ cƒÉn c·ª© d·ªØ li·ªáu r√µ r√†ng
    - Ph√¢n t√≠ch chuy√™n s√¢u nh∆∞ng d·ªÖ hi·ªÉu
    - Kh√¥ng tr·∫£ l·ªùi n·∫øu thi·∫øu th√¥ng tin, tr√°nh suy ƒëo√°n v√¥ cƒÉn c·ª©

    ‚ùó L∆∞u √Ω quan tr·ªçng:
    N·∫øu kh√¥ng c√≥ ƒë·ªß d·ªØ li·ªáu ho·∫∑c th√¥ng tin r√µ r√†ng, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥. Tuy·ªát ƒë·ªëi kh√¥ng ph·ªèng ƒëo√°n ho·∫∑c b·ªãa ƒë·∫∑t.
    """

    
    formatted_prompt = f"""
        B·∫°n l√† m·ªôt tr·ª£ l√Ω giao th√¥ng th√¥ng minh. D·ª±a v√†o c√°c th√¥ng tin ƒë√£ ƒë∆∞·ª£c thu th·∫≠p, h√£y tr·∫£ l·ªùi ch√≠nh x√°c c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
        # C√¢u h·ªèi:
        {user_input}

        # D·ªØ li·ªáu li√™n quan:
        {context}
    """
    
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": formatted_prompt}]

    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
        padding=True,
    ).to("cuda")

    attention_mask = (inputs != tokenizer.pad_token_id).long()
    
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)
    generation_kwargs = {
        "inputs": inputs,
        "streamer": streamer,
        "max_new_tokens": 512,
        "temperature": 1.5,
        "min_length": 30,  
        "use_cache": True,
        "top_p": 0.95,
        "min_p": 0.1,
        "attention_mask" : attention_mask,
    }
    
    # Start the generation in a separate thread
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    
    # Stream the generated text
    for _, new_text in enumerate(streamer):
        if "<|eot_id|>" in new_text:
            new_text = new_text.replace("<|eot_id|>", "")
        print(new_text)
        yield new_text
        time.sleep(0.02)

def main(static_retriever):
    st.markdown(
        "<style>hr {display: none !important;}</style>",
        unsafe_allow_html=True
    )
    st.image("./resources/Background.png")
    st.title("Chat with TrafficChat ViVi")
    st.write("Nh·∫≠p vƒÉn b·∫£n v√† m√¥ h√¨nh s·∫Ω tr·∫£ l·ªùi.")

    if 'model' not in st.session_state or 'tokenizer' not in st.session_state:
        with st.spinner("ƒêang t·∫£i m√¥ h√¨nh..."):
            st.session_state.model, st.session_state.tokenizer = load_model()
    
    # Nh·∫≠p t·ª´ ng∆∞·ªùi d√πng
    user_input = st.text_area("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:")

    # Run real - time from database
    export_data_for_chatbot(txt_directory)
    txt_documents = load_documents_from_directory(txt_directory)
    txt_chunks = semantic_chunk_real_time(hf_embedding, txt_documents)
    realtime_faiss, txt_chunks = build_realtime_faiss_index(hf_embedding, txt_chunks)
    realtime_bm25 = Bm25_Retriever(txt_chunks)
    
    realtime_retriever = Retriever(
        semantic_retriever = realtime_faiss, 
        bm25_retriever = realtime_bm25, 
        reranker = reranker(), 
        documents = txt_documents)
    if st.button("G·ª≠i"):
        if user_input:
            with st.spinner("ƒêang x·ª≠ l√Ω..."):
                user_input = user_input.replace('\n','')
                context = get_answer_with_two_retrievers(user_input, realtime_retriever, static_retriever, reranker(), 10)
                st.write_stream(generate_response_streaming_from_prompt_format(st.session_state.model, st.session_state.tokenizer, user_input, context))
        else:
            st.error("Vui l√≤ng nh·∫≠p c√¢u h·ªèi.")

if __name__ == "__main__":
    
    list_of_chunks = load_chunk("vectorstore/db_document/documents.pkl")
    vector_db_path = 'vectorstore/db_faiss'
    json_directory = "Data_Traffic/JSON"
    pdf_directory = "Data_Traffic/PDF"
    txt_directory = "Data_Traffic/TXT" 
    hf_embedding = load_embedding_model()

    # pdf_books = read_all_pdfs_with_plumber(pdf_directory)
    # pdf_documents = convert_to_documents(pdf_books)
    pdf_documents = load_documents_from_json(os.path.join(pdf_directory, "processed_pdf_documents.json"))
    json_documents = load_all_json_files_in_directory(json_directory)
    documents = json_documents + pdf_documents
    # documents = json_documents

    semantic_results = load_Faiss_index(hf_embedding, vector_db_path)
    bm25_retriever = Bm25_Retriever(list_of_chunks)

    static_retriever = Retriever(
        semantic_retriever = semantic_results, 
        bm25_retriever = bm25_retriever, 
        reranker = reranker(), 
        documents = documents)
    
    main(static_retriever)