# langgraph_agent.py

from langgraph.graph import StateGraph, END
from typing import Dict, TypedDict, List, Optional
from threading import Thread
import os, sys, time
from langchain.tools.tavily_search import TavilySearchResults
from langchain_core.tools import Tool
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda

# CÃ¡c module Ä‘Ã£ cÃ³
# ThÃªm Ä‘Æ°á»ng dáº«n thÆ° má»¥c cha cá»§a AI_Agent vÃ o sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from model_utils import *
from RAG_Process.rag_process import *

# ===== Load Model & Search Tool =====
model, tokenizer = load_model()
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
# Load biáº¿n mÃ´i trÆ°á»ng tá»« file .env
load_dotenv()
## Set up API key
TAVILY_API_KEY=os.environ.get("TAVILY_API_KEY")
## Set up search tool
search_tool = TavilySearchResults(tavily_api_key=TAVILY_API_KEY, max_results=2)

# ===== Agent State Type =====
class AgentState(TypedDict):
    question: str
    context: str
    answer: str
    feedback: str
    feedback_history: List[Dict[str, str]] # NEW: lÆ°u danh sÃ¡ch pháº£n há»“i dáº¡ng {"feedback": ..., "classification": ...}
    index_static: int
    index_realtime: int
    memory: List[str]
    prompt_version: Optional[int]

# ===== Node: Search Tool (ReAct style) =====
def search_node(state: AgentState) -> AgentState:
    query = state["question"]
    results = search_tool.invoke({"query": query})
    docs = [r["content"] for r in results]
    context = "\n".join(docs)
    print("ğŸŒ Search Tool Context Loaded")
    return {"context": context}

# ====== Node: Dual Retriever (Real-Time + Static) ======
def retrieve_dual_node(state: AgentState) -> AgentState:
    query = state["question"]
    realtime_docs, index_rt = realtime_retriever(query)
    static_docs, index_st = static_retriever(query)
    print(f"Real-time index: {index_rt} - Static index: {index_st}")

    # Chá»n theo Ä‘á»™ Æ°u tiÃªn
    if index_rt == 4 and index_st == 4:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin.")
        return {"context": [], "index_static": 4, "index_realtime": 4}

    if index_rt < index_st:
        context_docs = realtime_docs
    elif index_st < index_rt:
        context_docs = static_docs
    else:
        merged_docs = realtime_docs + static_docs
        # Rerank náº¿u cÃ³
        if reranker:
            pairs = [[query, doc.page_content] for doc in merged_docs]
            scores = reranker.compute_score(pairs, normalize=True)
            for i in range(len(merged_docs)):
                merged_docs[i].metadata['score'] = scores[i]
            merged_docs = sorted(merged_docs, key=lambda x: x.metadata['score'], reverse=True)
        context_docs = merged_docs[:5]  # láº¥y top 5

    # Gá»™p vÄƒn báº£n láº¡i
    context_str = "\n".join([doc.page_content for doc in context_docs])
    return {
        "context": context_str,
        "index_static": index_st,
        "index_realtime": index_rt
    }

# ===== Node: Generate LLM Response =====
def generate_llm_node(state: AgentState) -> AgentState:
    question = state["question"]
    context = state["context"]
    memory = state.get("memory", [])
    prompt_version = state.get("prompt_version", 1)
    feedback_history = state.get("feedback_history", [])

    # Náº¿u cÃ³ nhiá»u pháº£n há»“i tiÃªu cá»±c, nháº¯c nhá»Ÿ LLM rÃµ rÃ ng hÆ¡n
    recent_feedbacks = feedback_history[-2:] if feedback_history else []
    bad_count = sum(1 for fb in recent_feedbacks if fb["classification"] == "tiÃªu cá»±c")
    # Gáº¯n thÃªm cáº£nh bÃ¡o náº¿u ngÆ°á»i dÃ¹ng tá»«ng khÃ´ng hÃ i lÃ²ng
    if bad_count >= 2:
        last_feedback = recent_feedbacks[-1]["feedback"] if recent_feedbacks else ""
        context += (
            "\n**LÆ°u Ã½ Ä‘áº·c biá»‡t cho trá»£ lÃ½:**\n"
            "- NgÆ°á»i dÃ¹ng Ä‘Ã£ khÃ´ng hÃ i lÃ²ng vá»›i cÃ¡c pháº£n há»“i trÆ°á»›c.\n"
            f"- Ná»™i dung pháº£n há»“i gáº§n nháº¥t: \"{last_feedback}\"\n"
            "- HÃ£y Ä‘áº£m báº£o tráº£ lá»i rÃµ rÃ ng, cá»¥ thá»ƒ, vÃ  trÃ¡nh phá»ng Ä‘oÃ¡n náº¿u thiáº¿u dá»¯ liá»‡u.\n"
        )

    system_prompt = """
        Báº¡n lÃ  ViVi â€“ má»™t nhÃ  quy hoáº¡ch giao thÃ´ng Ä‘Ã´ thá»‹ thÃ´ng minh.
        ğŸ¯ Vai trÃ²: PhÃ¢n tÃ­ch dá»¯ liá»‡u & Ä‘á» xuáº¥t giáº£i phÃ¡p quy hoáº¡ch giao thÃ´ng Ä‘Ã´ thá»‹.
        ğŸ“Œ LÆ°u Ã½: KhÃ´ng phá»ng Ä‘oÃ¡n náº¿u thiáº¿u thÃ´ng tin. LuÃ´n rÃµ rÃ ng, dá»±a trÃªn dá»¯ liá»‡u.
    """

    reasoning_part = """
        HÃ£y suy nghÄ© tá»«ng bÆ°á»›c, Ä‘Ã¡nh giÃ¡ dá»¯ liá»‡u, láº­p káº¿ hoáº¡ch náº¿u cáº§n. Dáº¡ng: (Suy nghÄ© -> HÃ nh Ä‘á»™ng -> Quan sÃ¡t).
    """

    prompt_v1 = f"""
        # CÃ¢u há»i:
        {question}

        # Dá»¯ liá»‡u:
        {context}

        # Lá»‹ch sá»­:
        {" | ".join(memory[-3:]) if memory else "KhÃ´ng cÃ³."}

        {reasoning_part if prompt_version > 1 else ""}
    """

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt_v1}
    ]

    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
        padding=True
    ).to("cuda")

    attention_mask = (inputs != tokenizer.pad_token_id).long()
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)

    generation_kwargs = {
        "inputs": inputs,
        "streamer": streamer,
        "max_new_tokens": 512,
        "temperature": 1.2,
        "top_p": 0.95,
        "min_length": 30,
        "attention_mask": attention_mask
    }

    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    full_response = ""
    for chunk in streamer:
        if "<|eot_id|>" in chunk:
            chunk = chunk.replace("<|eot_id|>", "")
        full_response += chunk
        print(chunk, end="", flush=True)
        time.sleep(0.01)

    return {"answer": full_response}

# ===== Node: Evaluate Feedback & Prompt Repair =====
def rewrite_prompt_with_llm(old_question, feedback):
    prompt = (
        "Báº¡n lÃ  má»™t chuyÃªn gia NLP, chuyÃªn sá»­a cÃ¢u há»i khÃ´ng rÃµ rÃ ng.\n"
        f"NgÆ°á»i dÃ¹ng há»i: \"{old_question}\"\n"
        f"Pháº£n há»“i cá»§a há»: \"{feedback}\"\n"
        "â†’ Viáº¿t láº¡i cÃ¢u há»i sao cho rÃµ rÃ ng, chÃ­nh xÃ¡c vÃ  dá»… hiá»ƒu hÆ¡n."
    )

    response = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7)[0]["generated_text"]
    rewritten = response.replace(prompt, "").strip()  # Loáº¡i bá» prompt Ä‘áº§u náº¿u cáº§n
    return rewritten

def feedback_node(state: AgentState) -> AgentState:
    feedback = state.get("feedback", "").lower()
    memory = state.get("memory", [])
    answer = state.get("answer", "")
    question = state["question"]
    prompt_version = state.get("prompt_version", 1)
    feedback_history = state.get("feedback_history", [])

    print(f"ğŸ“¨ Feedback: {feedback}")
    # Prompt Ä‘á»ƒ nhá» LLM Ä‘Ã¡nh giÃ¡ pháº£n há»“i
    feedback_prompt = f"""
        Báº¡n lÃ  má»™t chuyÃªn gia Ä‘Ã¡nh giÃ¡ pháº£n há»“i ngÆ°á»i dÃ¹ng.
        CÃ¢u há»i: {question}
        CÃ¢u tráº£ lá»i: {answer}
        Pháº£n há»“i cá»§a ngÆ°á»i dÃ¹ng: {feedback}
        â†’ Pháº£n há»“i lÃ  tÃ­ch cá»±c hay tiÃªu cá»±c? Tráº£ lá»i má»™t tá»«: "tÃ­ch cá»±c" hoáº·c "tiÃªu cá»±c".
    """

    result = pipe(feedback_prompt, max_new_tokens=10, do_sample=False)[0]["generated_text"]
    classification = result.replace(feedback_prompt, "").strip().lower()
    print(f"ğŸ¤– LLM Ä‘Ã¡nh giÃ¡ pháº£n há»“i: {classification}")

    # Cáº­p nháº­t feedback history
    feedback_history.append({"feedback": feedback, "classification": classification})

    if "tiÃªu cá»±c" in classification:
        print("â— Pháº£n há»“i cho biáº¿t tráº£ lá»i sai â†’ Ghi log & sinh prompt má»›i")
        with open("AI_Agent/log_failed_cases.txt", "a", encoding="utf-8") as f:
            f.write(f"[âŒ] Q: {state['question']}\nA: {answer}\nFeedback: {feedback}\n---\n")
        
        # Gá»£i Ã½ cÃ¢u há»i viáº¿t láº¡i tá»« LLM náº¿u pháº£n há»“i quÃ¡ nhiá»u láº§n
        prompt_version += 1
        if prompt_version >= 3:
            print("âš ï¸ LiÃªn tiáº¿p tráº£ lá»i sai. Chá»‰nh sá»­a method tiáº¿p cáº­n Ä‘á»ƒ solve problem")
            # Tá»± Ä‘á»™ng sá»­a cÃ¢u há»i báº±ng LLM (Prompt Repair)
            rewritten_question = rewrite_prompt_with_llm(state['question'], feedback)
            print("ğŸ” Prompt má»›i Ä‘Æ°á»£c táº¡o láº¡i:", rewritten_question)
            return {
                "question": rewritten_question,
                "memory": memory + [question],
                "prompt_version": prompt_version,
                "feedback_history": feedback_history,
                "next_step": "generate"
            }
        
        return {
            "memory": memory + [question],
            "prompt_version": prompt_version,
            "feedback_history": feedback_history,
            "next_step": "generate"  # âœ… Ä‘Ã¢y lÃ  Ä‘iá»u kiá»‡n branch
        }
    
    else:
        print("âœ… Pháº£n há»“i tá»‘t.")
        return {
            "feedback_history": feedback_history,
            "next_step": "end" # âœ… máº·c Ä‘á»‹nh lÃ  káº¿t thÃºc
        }  

# ===== LangGraph: Build the Agent =====
graph = StateGraph(AgentState)

graph.add_node("search", search_node)
graph.add_node("retrieve", retrieve_dual_node)
graph.add_node("generate", generate_llm_node)
graph.add_node("feedback", feedback_node)

# Entry â†’ retrieve (RAG) â†’ generate â†’ feedback â†’ END
graph.set_entry_point("retrieve")
graph.add_edge("retrieve", "generate")
graph.add_edge("generate", "feedback")
graph.add_conditional_edges("feedback", lambda state: state.get("next_step", "end"), {
    "generate": "generate",
    "end": END
})

agent_app = graph.compile()
